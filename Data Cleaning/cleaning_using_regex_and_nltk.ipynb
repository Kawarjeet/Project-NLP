{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "cleaning_using_regex_and_nltk",
      "provenance": [],
      "authorship_tag": "ABX9TyPONsgLjgnhqKa/D2cbX2yw",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kawarjeet/Project-NLP/blob/main/Data%20Cleaning/cleaning_using_regex_and_nltk.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning our Text Data using Regular Expression and NLTK"
      ],
      "metadata": {
        "id": "aU6RcYjY6kd_"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To clean our text data, let us take a simple case in which the text data file is pretty much clean. We open the file using inbuilt open() python function which takes in two parameters in our case. \n",
        "- 'rt' : the 'r' stands for read mode and 't' for text format. \n",
        "- we could have opened the file in binary mode too. To do that the parameter would have changed to 'rb' inplace of 'rt'\n"
      ],
      "metadata": {
        "id": "KvIyYk3xrrAH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "1PGqwADnoG3P"
      },
      "outputs": [],
      "source": [
        "file = open('poem.txt', 'rt') "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next we read the data in the text file into a variable using the python read() function. We could have also specified the number of bytes we would have wanted to read as read(33) - reads first 33 bytes from the text file. Note, one character is one byte."
      ],
      "metadata": {
        "id": "O4mxGM6Wso4U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = file.read()"
      ],
      "metadata": {
        "id": "CJPasY0WsgiH"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "file.close()"
      ],
      "metadata": {
        "id": "x2dhbmLip1ze"
      },
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9wVGv0lRp3Qp",
        "outputId": "88eca67a-8cb2-4e07-f652-06b2d9443637"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            " by Manuel Gutiérrez Nájera\n",
            "\n",
            "I want to die as the day declines, \n",
            "at high sea and facing the sky, \n",
            "while agony seems like a dream \n",
            "and my soul like a bird that can fly. \n",
            "\n",
            "To hear not, at this last moment, \n",
            "once alone with sky and sea, \n",
            "any more voices nor weeping prayers \n",
            "than the majestic beating of the waves. \n",
            "\n",
            "To die when the sad light retires \n",
            "its golden network from the green waves \n",
            "to be like the sun that slowly expires; \n",
            "something very luminous that fades. \n",
            "\n",
            "To die, and die young, before \n",
            "fleeting time removes the gentle crown, \n",
            "while life still says: \"I'm yours\" \n",
            "though we know with our hearts that she lies. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words  = text.split()"
      ],
      "metadata": {
        "id": "clRNgsnUp5c8"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAeCvOzzqCwQ",
        "outputId": "5e0595a9-57ef-4c01-eaaf-2a9a13beff46"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'to', 'die', 'as', 'the', 'day', 'declines,', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky,', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly.', 'To', 'hear', 'not,', 'at', 'this', 'last', 'moment,', 'once', 'alone', 'with', 'sky', 'and', 'sea,', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves.', 'To', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires;', 'something', 'very', 'luminous', 'that', 'fades.', 'To', 'die,', 'and', 'die', 'young,', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown,', 'while', 'life', 'still', 'says:', '\"I\\'m', 'yours\"', 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we can see that the issue with a simple split() function is that it is separating the words by white space and preserves punctuations like comma. "
      ],
      "metadata": {
        "id": "hM1IPSt0tuWZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- To over come this we could use regex model which separates strings of alphanumeric characters( a-z, A-Z, 0-9 and '_' ) \n",
        "- We need to import the 're' library to use the regex model.\n",
        "- The \\W is a regex special sequence that matches any Non-alphanumeric character."
      ],
      "metadata": {
        "id": "r-6zHmvCtsHs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "words_using_regex_split = re.split(r'\\W+', text)\n",
        "print(words_using_regex_split)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uwNclm4eqTfQ",
        "outputId": "b3c3156c-a162-4ee5-bfba-e3230b685621"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', 'by', 'Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'to', 'die', 'as', 'the', 'day', 'declines', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly', 'To', 'hear', 'not', 'at', 'this', 'last', 'moment', 'once', 'alone', 'with', 'sky', 'and', 'sea', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves', 'To', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires', 'something', 'very', 'luminous', 'that', 'fades', 'To', 'die', 'and', 'die', 'young', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown', 'while', 'life', 'still', 'says', 'I', 'm', 'yours', 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "words_using_regex_split.count(\"like\") # we see that number of 'like' string are 3."
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7yHMk5FTq2OF",
        "outputId": "8700636e-7753-4fb1-d3b1-b9e933abc886"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- However, regex is splitting \"What's\" string as 'What' and 's' which is not great. So we could use the combination of split() function and the re.split() function to achieve our result. The result that we are looking for is words without and white space and without any punctuations. Let's look at a constant called 'strin.punctuation'. Let's explore it. But before using it we have to import the string library. "
      ],
      "metadata": {
        "id": "jg4GM1hPwu10"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "print(string.punctuation)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kHsTa5BwwhyR",
        "outputId": "dff871c4-a42e-4fdf-fc22-7111517ca6df"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))"
      ],
      "metadata": {
        "id": "WRpLkzHaxeSD"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "stripped = [re_punc.sub('', w) for w in words]"
      ],
      "metadata": {
        "id": "pYz0u2YFqxaq"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(stripped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xvIqqy9bqPO-",
        "outputId": "7c55e7c7-60ce-4db6-d4de-a664e75b6cb5"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'to', 'die', 'as', 'the', 'day', 'declines', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly', 'To', 'hear', 'not', 'at', 'this', 'last', 'moment', 'once', 'alone', 'with', 'sky', 'and', 'sea', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves', 'To', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires', 'something', 'very', 'luminous', 'that', 'fades', 'To', 'die', 'and', 'die', 'young', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown', 'while', 'life', 'still', 'says', 'Im', 'yours', 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- ***string.printable*** is another constant that separates non-printable characters from printable characters. "
      ],
      "metadata": {
        "id": "N_-fIj8G1oPi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(string.printable)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ej2MYlHa1gJ1",
        "outputId": "114508f3-fc35-4e16-e85a-8d86d6de1a17"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0123456789abcdefghijklmnopqrstuvwxyzABCDEFGHIJKLMNOPQRSTUVWXYZ!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~ \t\n",
            "\r\u000b\f\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "re_printable = re.compile('[%s]' % re.escape(string.printable))\n",
        "non_printable_stripped = [re_printable.sub('', w) for w in words]\n",
        "print(non_printable_stripped)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bR1bWFbs15Cj",
        "outputId": "c13c1773-b844-4ffb-aa6b-c85018470e53"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['', '', 'é', 'á', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '', '']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we can see, the above query prints all the non printable characters in the array. Let us remove all the unnecessary elements from this. "
      ],
      "metadata": {
        "id": "Yk4b5fgt3EG-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "non_printable_stripped1 = [i for i in non_printable_stripped if i!='' ]\n",
        "print(non_printable_stripped1)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A6LnIusN2_j1",
        "outputId": "2d090665-7b45-49f2-8c62-998b64b780e2"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['é', 'á']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Next we convert all of our words into lower case using the lower() function. Same is the case with upper case."
      ],
      "metadata": {
        "id": "kgJdrJiH5K9o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lowered_words = [words.lower() for words in stripped]\n",
        "upper_words = [words.upper() for words in stripped]\n",
        "print(lowered_words)\n",
        "print(upper_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onTdMq2E4Mln",
        "outputId": "05de6263-3c02-4280-8ac9-b073fa47c17c"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'manuel', 'gutiérrez', 'nájera', 'i', 'want', 'to', 'die', 'as', 'the', 'day', 'declines', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly', 'to', 'hear', 'not', 'at', 'this', 'last', 'moment', 'once', 'alone', 'with', 'sky', 'and', 'sea', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves', 'to', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires', 'something', 'very', 'luminous', 'that', 'fades', 'to', 'die', 'and', 'die', 'young', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown', 'while', 'life', 'still', 'says', 'im', 'yours', 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies']\n",
            "['BY', 'MANUEL', 'GUTIÉRREZ', 'NÁJERA', 'I', 'WANT', 'TO', 'DIE', 'AS', 'THE', 'DAY', 'DECLINES', 'AT', 'HIGH', 'SEA', 'AND', 'FACING', 'THE', 'SKY', 'WHILE', 'AGONY', 'SEEMS', 'LIKE', 'A', 'DREAM', 'AND', 'MY', 'SOUL', 'LIKE', 'A', 'BIRD', 'THAT', 'CAN', 'FLY', 'TO', 'HEAR', 'NOT', 'AT', 'THIS', 'LAST', 'MOMENT', 'ONCE', 'ALONE', 'WITH', 'SKY', 'AND', 'SEA', 'ANY', 'MORE', 'VOICES', 'NOR', 'WEEPING', 'PRAYERS', 'THAN', 'THE', 'MAJESTIC', 'BEATING', 'OF', 'THE', 'WAVES', 'TO', 'DIE', 'WHEN', 'THE', 'SAD', 'LIGHT', 'RETIRES', 'ITS', 'GOLDEN', 'NETWORK', 'FROM', 'THE', 'GREEN', 'WAVES', 'TO', 'BE', 'LIKE', 'THE', 'SUN', 'THAT', 'SLOWLY', 'EXPIRES', 'SOMETHING', 'VERY', 'LUMINOUS', 'THAT', 'FADES', 'TO', 'DIE', 'AND', 'DIE', 'YOUNG', 'BEFORE', 'FLEETING', 'TIME', 'REMOVES', 'THE', 'GENTLE', 'CROWN', 'WHILE', 'LIFE', 'STILL', 'SAYS', 'IM', 'YOURS', 'THOUGH', 'WE', 'KNOW', 'WITH', 'OUR', 'HEARTS', 'THAT', 'SHE', 'LIES']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Cleaning using NLTK"
      ],
      "metadata": {
        "id": "Vbz98ogX5-aT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Till now we were cleaning our text document using python split() function and regex model. Let us now dive into NLTK, which is nothing but Natural Language Toolkit. "
      ],
      "metadata": {
        "id": "dyVL-tdH5-TS"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the script below we use sent_tokenize to split text based on sentences that are separated by \\n escape sequence. "
      ],
      "metadata": {
        "id": "wdAovz0M98DD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "nltk.download('punkt')\n",
        "from nltk import sent_tokenize\n",
        "sentences = sent_tokenize(text)\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FBPincYA73Gg",
        "outputId": "4832a082-530a-4b0a-cabb-a0e9ad598849"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[' by Manuel Gutiérrez Nájera\\n\\nI want to die as the day declines, \\nat high sea and facing the sky, \\nwhile agony seems like a dream \\nand my soul like a bird that can fly.', 'To hear not, at this last moment, \\nonce alone with sky and sea, \\nany more voices nor weeping prayers \\nthan the majestic beating of the waves.', 'To die when the sad light retires \\nits golden network from the green waves \\nto be like the sun that slowly expires; \\nsomething very luminous that fades.', 'To die, and die young, before \\nfleeting time removes the gentle crown, \\nwhile life still says: \"I\\'m yours\" \\nthough we know with our hearts that she lies.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- The word_tokenize helps to separate the strings based on white spaces. Also, it takes care of the punctuations and separates them out to. But it does not separate in words punctuation like \"I'm\" effectively"
      ],
      "metadata": {
        "id": "04eRJ-AG-Q6v"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "words_tokenized_using_nltk = word_tokenize(text)\n",
        "print(words_tokenized_using_nltk)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vULJm5FJ8xdL",
        "outputId": "5a5574a8-ecfe-457e-ca35-a2ce8f851d64"
      },
      "execution_count": 48,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'to', 'die', 'as', 'the', 'day', 'declines', ',', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky', ',', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly', '.', 'To', 'hear', 'not', ',', 'at', 'this', 'last', 'moment', ',', 'once', 'alone', 'with', 'sky', 'and', 'sea', ',', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves', '.', 'To', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires', ';', 'something', 'very', 'luminous', 'that', 'fades', '.', 'To', 'die', ',', 'and', 'die', 'young', ',', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown', ',', 'while', 'life', 'still', 'says', ':', '``', 'I', \"'m\", 'yours', \"''\", 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies', '.']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Let us now separate out the punctuation from our result. We use isalpha() function to do that as shown in the below script. Note running the script not all rmoves punctuation tokens but also examples like \"'m\""
      ],
      "metadata": {
        "id": "G6lpXb_J-7CO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words_tokenized_without_punc = [word for word in words_tokenized_using_nltk if word.isalpha()]\n",
        "print(words_tokenized_without_punc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YXFQkDW-79v2",
        "outputId": "fb5299c5-f23d-446a-f8cf-954350508494"
      },
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['by', 'Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'to', 'die', 'as', 'the', 'day', 'declines', 'at', 'high', 'sea', 'and', 'facing', 'the', 'sky', 'while', 'agony', 'seems', 'like', 'a', 'dream', 'and', 'my', 'soul', 'like', 'a', 'bird', 'that', 'can', 'fly', 'To', 'hear', 'not', 'at', 'this', 'last', 'moment', 'once', 'alone', 'with', 'sky', 'and', 'sea', 'any', 'more', 'voices', 'nor', 'weeping', 'prayers', 'than', 'the', 'majestic', 'beating', 'of', 'the', 'waves', 'To', 'die', 'when', 'the', 'sad', 'light', 'retires', 'its', 'golden', 'network', 'from', 'the', 'green', 'waves', 'to', 'be', 'like', 'the', 'sun', 'that', 'slowly', 'expires', 'something', 'very', 'luminous', 'that', 'fades', 'To', 'die', 'and', 'die', 'young', 'before', 'fleeting', 'time', 'removes', 'the', 'gentle', 'crown', 'while', 'life', 'still', 'says', 'I', 'yours', 'though', 'we', 'know', 'with', 'our', 'hearts', 'that', 'she', 'lies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Removing stopwords from the text files\n"
      ],
      "metadata": {
        "id": "j05lKS6tAOH7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Next we learn how to remove the stopwords from our text file using NLTK library. "
      ],
      "metadata": {
        "id": "AZkcy-61AoCR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords\n",
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')\n",
        "print(stop_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FqF3ZxuO7417",
        "outputId": "e5b09d1b-7734-4c9c-d367-c946e16439e5"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_and_no_stopwords = [i for i in words_tokenized_without_punc if i not in stop_words]\n",
        "print(tokenized_and_no_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wbvQ-Hv8BXCm",
        "outputId": "7e7084a4-dc37-4725-bc1a-cfb06f702823"
      },
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['Manuel', 'Gutiérrez', 'Nájera', 'I', 'want', 'die', 'day', 'declines', 'high', 'sea', 'facing', 'sky', 'agony', 'seems', 'like', 'dream', 'soul', 'like', 'bird', 'fly', 'To', 'hear', 'last', 'moment', 'alone', 'sky', 'sea', 'voices', 'weeping', 'prayers', 'majestic', 'beating', 'waves', 'To', 'die', 'sad', 'light', 'retires', 'golden', 'network', 'green', 'waves', 'like', 'sun', 'slowly', 'expires', 'something', 'luminous', 'fades', 'To', 'die', 'die', 'young', 'fleeting', 'time', 'removes', 'gentle', 'crown', 'life', 'still', 'says', 'I', 'though', 'know', 'hearts', 'lies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we see that some of the stop words are not removed. This is because the stop words are all in lower case. So lets convert the words in lower case first and again apply the stop word removal method."
      ],
      "metadata": {
        "id": "iZEWUbUNBtba"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_and_no_stopwords = [i.lower() for i in tokenized_and_no_stopwords]\n",
        "print(tokenized_and_no_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ACyGHofWCCOI",
        "outputId": "c4e8e0cb-163c-4344-e4aa-1637a3ccc6c9"
      },
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['manuel', 'gutiérrez', 'nájera', 'i', 'want', 'die', 'day', 'declines', 'high', 'sea', 'facing', 'sky', 'agony', 'seems', 'like', 'dream', 'soul', 'like', 'bird', 'fly', 'to', 'hear', 'last', 'moment', 'alone', 'sky', 'sea', 'voices', 'weeping', 'prayers', 'majestic', 'beating', 'waves', 'to', 'die', 'sad', 'light', 'retires', 'golden', 'network', 'green', 'waves', 'like', 'sun', 'slowly', 'expires', 'something', 'luminous', 'fades', 'to', 'die', 'die', 'young', 'fleeting', 'time', 'removes', 'gentle', 'crown', 'life', 'still', 'says', 'i', 'though', 'know', 'hearts', 'lies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_and_no_stopwords = [ word for word in tokenized_and_no_stopwords if word not in stop_words]\n",
        "print(tokenized_and_no_stopwords)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J03OkUtZCSHD",
        "outputId": "ce68d118-3665-41bb-e251-9d1ca8fd0641"
      },
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['manuel', 'gutiérrez', 'nájera', 'want', 'die', 'day', 'declines', 'high', 'sea', 'facing', 'sky', 'agony', 'seems', 'like', 'dream', 'soul', 'like', 'bird', 'fly', 'hear', 'last', 'moment', 'alone', 'sky', 'sea', 'voices', 'weeping', 'prayers', 'majestic', 'beating', 'waves', 'die', 'sad', 'light', 'retires', 'golden', 'network', 'green', 'waves', 'like', 'sun', 'slowly', 'expires', 'something', 'luminous', 'fades', 'die', 'die', 'young', 'fleeting', 'time', 'removes', 'gentle', 'crown', 'life', 'still', 'says', 'though', 'know', 'hearts', 'lies']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- And there it is! We have removed all the non necessary words. "
      ],
      "metadata": {
        "id": "6sFuXKu_ChhN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "6ltQ8s_-Cyyv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- As we have seen, the pipeline in cleaning the text file is as follows: \n",
        "<ol>\n",
        "  <li>load the raw text</li>\n",
        "  <li>split the tokens</li>\n",
        "  <li>convert to lowercase</li>\n",
        "  <li>remove punctuation from the tokens</li>\n",
        "  <li>filter out remaining tokens that are not alpha numeric</li>\n",
        "  <li>Filter out tokens that are stopwords</li>\n",
        "</ol>"
      ],
      "metadata": {
        "id": "uhmJxq9ICylL"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Let us now work the entire learning in one script for a clean text document. "
      ],
      "metadata": {
        "id": "EtJnFKkwECgy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# importing the necessary libraries\n",
        "import nltk\n",
        "from nltk import word_tokenize\n",
        "import string"
      ],
      "metadata": {
        "id": "k3g0QZkRDD1f"
      },
      "execution_count": 67,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nltk.download('stopwords')\n",
        "stop_words = stopwords.words('english')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EbwxKWNZGY5I",
        "outputId": "c65e3570-2c2a-484b-9977-369f5ed8d0bc"
      },
      "execution_count": 68,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_file = open('poem.txt', 'rt')\n",
        "script = test_file.read()\n",
        "test_file.close()\n",
        "\n",
        "words = word_tokenize(script)\n",
        "re_punc = re.compile('[%s]' % re.escape(string.punctuation))\n",
        "punc_stripped_script = [re_punc.sub('',w) for w in words]\n",
        "low_punc_strip_tok_script = [word.lower() for word in punc_stripped_script ]\n",
        "low_punc_strip_tok_script = [word for word in low_punc_strip_tok_script if word!='']\n",
        "nostop_low_nopunc_tok = [ word for word in low_punc_strip_tok_script if word not in stop_words]\n",
        "print(len(words))\n",
        "print(len(nostop_low_nopunc_tok))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LXR-bcy8Coln",
        "outputId": "ab81c3a7-73ba-471b-aa83-ecd641b29374"
      },
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "131\n",
            "61\n"
          ]
        }
      ]
    }
  ]
}